{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021.02_09_ArvDecisão.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y73IQOnw7hWl"},"source":["Computa**Instituto de Informática - UFG**\n","\n","Disciplina: Inteligência Computacional\n","\n","Prof. Cedric Luiz de Carvalho\n","\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"KKz0LDRV71PO"},"source":["# Árvores de Decisão"]},{"cell_type":"markdown","metadata":{"id":"yob_V7zD78fi"},"source":["- Árvores de decisão são modelos estatísticos que utilizam um\n","treinamento supervisionado para a classificação e previsão de dados.\n","- Utilizam a estratégia de dividir para conquistar.\n","    - Um problema complexo é decomposto em subproblemas mais simples.\n","    - Esta estratégia é aplicada recursivamente a cada subproblema.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"urWOx7BA8bac"},"source":["- Uma árvore de decisão representa a disjunção de conjunções de\n","restrições nos valores dos atributos.\n","- Cada ramo na árvore é uma conjunção de condições.\n","- O conjunto de ramos na árvore é disjunto - **Forma Normal\n","Disjuntiva**.\n"]},{"cell_type":"markdown","metadata":{"id":"k_kYyseZ82OA"},"source":["- A partir de um conjunto de atributos, decide SIM ou NÃO.\n","- A precisão é diretamente proporcional à quantidade de exemplos.\n","\n","- **Função aprendida**: Representada por uma árvore de decisão.\n","- **Entrada**: Objeto ou situação (alvo) descrita por um conjunto de atributos.\n","- **Saída**: Decisão sobre o alvo SIM ou NÃO.\n","- **Objetivo**: Tomar decisões e classificar objetos (neste caso,\n","classificação booleana).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-FLKzo7T9yeP"},"source":["##Problemas apropriados\n","\n","- Instâncias descritas por pares atributo-valor.\n","- Função alvo utiliza valores discretos de saída.\n","- Representa conhecimento proposicional, onde descrições disjuntas são aplicadas.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X8mwBU_AAOii"},"source":["##Exemplo de árvore de decisão\n","\n"]},{"cell_type":"code","metadata":{"id":"kT_3rj77ZTAy"},"source":["# Carregar as  figuras('estruturaArvore.png','dadosTreinamento','entropia','ganho1', 'ganho2' e 'graficoTreinamento.png') para o ambiente virtual\n","\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpDUa9PhEMsy"},"source":["from IPython.display import Image, display\n","\n","display(Image(filename='estruturaArvore.png'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ClZeOEi5B2gY"},"source":["- Podemos usar a entropia e o ganho de informação na indução de uma árvore de decisão:\n","\n","  - A **entropia** mede a homogeneidade do conjunto de  exemplos (exemplos de treinamento).\n","  - O **ganho de informação** é a redução esperada na entropia, causada pela partição nos exemplos por um atributo.\n","\n","\n","             - Para um conjunto de exemplos S:\n","\n","$$Entropia(S) \\equiv -p_{\\oplus}log_{2}p_{\\oplus} - p_{\\ominus}log_{2}p_{\\ominus}$$\n","\n","\n","$$Ganho(S,A) \\equiv Entropia(S) - \\sum\\limits_{v\\in val(A)} \\frac{\\left|S_v\\right|}{\\left|S\\right|}Entropia(S_v)$$\n","\n","\n","\n"]},{"cell_type":"code","source":["display(Image(filename='entropia.png'))"],"metadata":{"id":"1_LQPO4VFI3-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1y2ykvdG7p-"},"source":["\n","  - O **índice Gini** também pode ser usado. Ele mede o grau de heterogeneidade dos dados.\n","\n","$$I_G(p) = \\sum{p_i(1 - p_i)} = 1 - \\sum{p_i^2}$$\n","\n","       onde p_i é a frequêndia relativa de cada classe em cada nó.\n","\n","- Quando este índice é igual a zero, o nó é puro. Por outro lado, quando ele se aproxima   do   valor   um,   o   nó   é   impuro   (aumenta   o   número   de   classes    uniformemente distribuídas neste nó). \n","- Selecionamos uma partição que minimiza a impureza Gini para os nós filhos.\n","\n","- O ganho de informação será:\n","\n","$$H(p) = -\\sum{p_i \\log_2{p_i}}$$"]},{"cell_type":"markdown","source":["**Exemplo:**"],"metadata":{"id":"Kfrx6N9fFTC0"}},{"cell_type":"code","source":["display(Image(filename='dadosTreinamento.png'))"],"metadata":{"id":"Pw68hSKHFPwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Image(filename='ganho2.png'))"],"metadata":{"id":"kVagcpoDFf-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Image(filename='ganho1.png'))"],"metadata":{"id":"HgSsm4R0FiBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OotqdI6JViYy"},"source":["# Algoritmos mais conhecidos\n","\n","Estes algoritmos adotam uma abordagem gulosa (i.e.  sem *backtracking*), de forma *top-down*, recursiva, sob a estratégia \"dividir para conquistar\"."]},{"cell_type":"markdown","metadata":{"id":"ZbCw_wp9VmvT"},"source":["###ID3 (*Iterative Dichotomise*)\n","\n","\n","   \n","1.    Começar com todos os exemplos de treino;\n","2.    Escolher o atributo que melhor divide os exemplos - usar a entropia e o ganho de informação;\n","3.    Para o atributo escolhido, criar um nó filho para cada valor possível do atributo;\n","4.    Transportar os exemplos para cada filho tendo em conta o valor do filho;\n","5.     Repetir o procedimento para cada filho para o qual cada atributo tem mais de um valor no conjunto de exemplos.\n"]},{"cell_type":"markdown","metadata":{"id":"3q_qei2RYkVp"},"source":["##C4.5\n","\n","Este algoritimo é um aprimoramento do ID3, por ser capaz de lidar com dados indisponíveis, com valores contínuos, poda na árvore gerada e  a derivação de regras."]},{"cell_type":"markdown","metadata":{"id":"rXwlwERLbRmD"},"source":["##CART (*Classification And Regression Trees*)\n","\n","Usa a mesma metodologia de \"dividir para conquistar\" do algoritmo C4.5. A diferença está na estrutura da árvore (só trata de árvores com divisões binárias), no critério de particionamento (usa o índice Gini), no método de poda (poda de complexidade de mínimo custo) e na forma comos os valores ausentes são tratados (usa somente instâncias dos exemplos com valores conhecidos).\n","\n","\n","\n","1.   Definir o conjunto de regras para dividir o nó em dois nós filhos (as perguntas podem ser apenas aquelas que tem resposta do tipo 'sim' ou 'não')\n","2.   Encontra a melhor divisão pelo critério Gini.\n","3.   Repetir o processo até que novas divisões sem impossível ou que seja interrompido.\n","4. Aplicar o processo de pós-poda para encontrar a árvore de menor custo (aquela com menor taxa de erro e de menor complexidade)\n","\n"]},{"cell_type":"code","metadata":{"id":"6F-HV17h8PjY"},"source":["# Carregar os arquivos utils.py learning.py para o ambiente virtual\n","\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHGMKEyEVWY3"},"source":["#criar uma pasta para guardar as bases de dados\n","\n","!mkdir aima-data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xjhr6VYGKFGV"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q24wTTkPXrIa"},"source":["# Carregar as bases de dados (iris.csv, orings.csv,restaurant.csv,zoo.csv) para o ambiente virtual\n"," \n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UnQ423XXgYR"},"source":["#mover as bases de dados para a pasta de dados\n","!mv *.csv aima-data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VG6JSU6_Unwn"},"source":["from learning import *\n","#from notebook import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1VYCNEBR-ap"},"source":["iris = DataSet(name=\"iris\")\n","\n","DTL = DecisionTreeLearner(iris)\n","print(DTL([2.5, 1.9, 1.5, 1.1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Problema**:  *overfiting* (sobreajuste)"],"metadata":{"id":"iYgyhAahIQc2"}},{"cell_type":"markdown","source":["Como evitar?\n","\n","1.   Parar de crescer a árvore antes de alcançar o ponto de classicação\n","2.   Desenvolver uma árvore completa e então fazer uma poda.\n","\n"],"metadata":{"id":"3_DDf5jiIik3"}},{"cell_type":"markdown","source":["**Validação cruzada**: \n","\n","\n","*   Um algoritmo de aprendizagem é bom se produz hipóteses que fazem um\n","bom trabalho de previsão das classicações de exemplos não vistos.\n","\n"],"metadata":{"id":"VGp1QDToIxP0"}},{"cell_type":"markdown","source":["  1. Colecione um conjunto grande de exemplos;\n","  2. Divida em 2 conjuntos disjuntos:\n","    * conjunto de treinamento\n","    * conjunto de teste\n","  3. Use o algoritmo de aprendizado com o conjunto de treinamento para\n","gerar a hipóteses h.\n","  4. Calcule a percentagem de exemplos no conjunto de teste que estão\n","corretamente classicados por h.\n","  5. Repita os passos 1 a 4 para diferentes conjuntos."],"metadata":{"id":"mZTesn1HJF8w"}},{"cell_type":"code","source":["display(Image(filename='graficoTreinamento.png'))"],"metadata":{"id":"enCu96qiJoFW"},"execution_count":null,"outputs":[]}]}